{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31ec45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Trigram Language Model Implementation\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Read in the names dataset\n",
    "with open('names.txt', 'r') as f:\n",
    "    words = f.read().splitlines()\n",
    "\n",
    "# Let's first explore the dataset\n",
    "print(f\"Number of names: {len(words)}\")\n",
    "print(f\"Some example names: {words[:10]}\")\n",
    "\n",
    "# Split the dataset into train, dev, and test sets (E02)\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n = len(words)\n",
    "train_words = words[:int(n*0.8)]\n",
    "dev_words = words[int(n*0.8):int(n*0.9)]\n",
    "test_words = words[int(n*0.9):]\n",
    "print(f\"Train set size: {len(train_words)}\")\n",
    "print(f\"Dev set size: {len(dev_words)}\")\n",
    "print(f\"Test set size: {len(test_words)}\")\n",
    "\n",
    "# Let's build our vocabulary\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0 # add start/end token\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary: {itos}\")\n",
    "\n",
    "# -------------------------\n",
    "# E01: Trigram model (counting-based approach)\n",
    "# -------------------------\n",
    "\n",
    "# Build a trigram model that takes two characters to predict the third\n",
    "def build_trigram_model(words):\n",
    "    # Count the occurrences of character trigrams\n",
    "    trigram_counts = {}\n",
    "    for w in words:\n",
    "        chs = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            if (ch1, ch2) not in trigram_counts:\n",
    "                trigram_counts[(ch1, ch2)] = {}\n",
    "            if ch3 not in trigram_counts[(ch1, ch2)]:\n",
    "                trigram_counts[(ch1, ch2)][ch3] = 0\n",
    "            trigram_counts[(ch1, ch2)][ch3] += 1\n",
    "    return trigram_counts\n",
    "\n",
    "def calculate_trigram_loss(trigram_counts, words, smoothing=1.0):\n",
    "    # Calculate negative log likelihood loss\n",
    "    total_log_likelihood = 0\n",
    "    total_chars = 0\n",
    "    \n",
    "    for w in words:\n",
    "        chs = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            # If we've seen this bigram context before\n",
    "            if (ch1, ch2) in trigram_counts:\n",
    "                # Get count of this specific trigram\n",
    "                count = trigram_counts[(ch1, ch2)].get(ch3, 0)\n",
    "                # Get total count of all trigrams with this context\n",
    "                total = sum(trigram_counts[(ch1, ch2)].values())\n",
    "                # Calculate probability with smoothing\n",
    "                p = (count + smoothing) / (total + smoothing * vocab_size)\n",
    "            else:\n",
    "                # If we've never seen this context, use uniform distribution with smoothing\n",
    "                p = smoothing / (smoothing * vocab_size)\n",
    "            \n",
    "            total_log_likelihood += -math.log(p)\n",
    "            total_chars += 1\n",
    "    \n",
    "    return total_log_likelihood / total_chars\n",
    "\n",
    "# Build the trigram model on train set\n",
    "trigram_model = build_trigram_model(train_words)\n",
    "\n",
    "# Also build a bigram model for comparison\n",
    "def build_bigram_model(words):\n",
    "    # Count the occurrences of character bigrams\n",
    "    bigram_counts = {}\n",
    "    for w in words:\n",
    "        chs = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2 in zip(chs, chs[1:]):\n",
    "            if ch1 not in bigram_counts:\n",
    "                bigram_counts[ch1] = {}\n",
    "            if ch2 not in bigram_counts[ch1]:\n",
    "                bigram_counts[ch1][ch2] = 0\n",
    "            bigram_counts[ch1][ch2] += 1\n",
    "    return bigram_counts\n",
    "\n",
    "def calculate_bigram_loss(bigram_counts, words, smoothing=1.0):\n",
    "    # Calculate negative log likelihood loss\n",
    "    total_log_likelihood = 0\n",
    "    total_chars = 0\n",
    "    \n",
    "    for w in words:\n",
    "        chs = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2 in zip(chs, chs[1:]):\n",
    "            # If we've seen this character before\n",
    "            if ch1 in bigram_counts:\n",
    "                # Get count of this specific bigram\n",
    "                count = bigram_counts[ch1].get(ch2, 0)\n",
    "                # Get total count of all bigrams with this context\n",
    "                total = sum(bigram_counts[ch1].values())\n",
    "                # Calculate probability with smoothing\n",
    "                p = (count + smoothing) / (total + smoothing * vocab_size)\n",
    "            else:\n",
    "                # If we've never seen this character, use uniform distribution with smoothing\n",
    "                p = smoothing / (smoothing * vocab_size)\n",
    "            \n",
    "            total_log_likelihood += -math.log(p)\n",
    "            total_chars += 1\n",
    "    \n",
    "    return total_log_likelihood / total_chars\n",
    "\n",
    "# Build the bigram model on train set\n",
    "bigram_model = build_bigram_model(train_words)\n",
    "\n",
    "# Calculate losses on train, dev, and test sets\n",
    "print(\"Counting-based models evaluation:\")\n",
    "print(\"--------------------------------\")\n",
    "print(f\"Bigram train loss: {calculate_bigram_loss(bigram_model, train_words):.4f}\")\n",
    "print(f\"Trigram train loss: {calculate_trigram_loss(trigram_model, train_words):.4f}\")\n",
    "print(f\"Bigram dev loss: {calculate_bigram_loss(bigram_model, dev_words):.4f}\")\n",
    "print(f\"Trigram dev loss: {calculate_trigram_loss(trigram_model, dev_words):.4f}\")\n",
    "print(f\"Bigram test loss: {calculate_bigram_loss(bigram_model, test_words):.4f}\")\n",
    "print(f\"Trigram test loss: {calculate_trigram_loss(trigram_model, test_words):.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# E03: Tune smoothing strength for trigram model\n",
    "# -------------------------\n",
    "\n",
    "smoothing_values = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "\n",
    "print(\"\\nTuning smoothing parameter for trigram model:\")\n",
    "print(\"--------------------------------------------\")\n",
    "for smoothing in smoothing_values:\n",
    "    train_loss = calculate_trigram_loss(trigram_model, train_words, smoothing)\n",
    "    dev_loss = calculate_trigram_loss(trigram_model, dev_words, smoothing)\n",
    "    train_losses.append(train_loss)\n",
    "    dev_losses.append(dev_loss)\n",
    "    print(f\"Smoothing: {smoothing}, Train Loss: {train_loss:.4f}, Dev Loss: {dev_loss:.4f}\")\n",
    "\n",
    "# Find the best smoothing value based on dev loss\n",
    "best_smoothing_idx = np.argmin(dev_losses)\n",
    "best_smoothing = smoothing_values[best_smoothing_idx]\n",
    "print(f\"\\nBest smoothing value: {best_smoothing}\")\n",
    "\n",
    "# Evaluate on test set with best smoothing\n",
    "test_loss = calculate_trigram_loss(trigram_model, test_words, best_smoothing)\n",
    "print(f\"Test loss with best smoothing: {test_loss:.4f}\")\n",
    "\n",
    "# Plot smoothing results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(smoothing_values, train_losses, 'b-o', label='Train Loss')\n",
    "plt.plot(smoothing_values, dev_losses, 'r-o', label='Dev Loss')\n",
    "plt.axvline(x=best_smoothing, color='g', linestyle='--', label=f'Best Smoothing: {best_smoothing}')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Smoothing Value')\n",
    "plt.ylabel('Negative Log Likelihood Loss')\n",
    "plt.title('Impact of Smoothing on Trigram Model Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------\n",
    "# Neural Network-based Trigram Model Implementation\n",
    "# -------------------------\n",
    "\n",
    "# Function to build dataset for training\n",
    "def build_dataset(words, context_size=2):\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        # Add start and end tokens\n",
    "        context = [0] * context_size  # Start with '.' tokens\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context.copy())  # Use the context as input\n",
    "            Y.append(ix)              # Predict the next character\n",
    "            # Update context by shifting and adding the new character\n",
    "            context = context[1:] + [ix]\n",
    "    return torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "# Create datasets\n",
    "X_train, Y_train = build_dataset(train_words, context_size=2)  # For trigram model\n",
    "X_dev, Y_dev = build_dataset(dev_words, context_size=2)\n",
    "X_test, Y_test = build_dataset(test_words, context_size=2)\n",
    "\n",
    "# We'll now implement our neural network-based trigram model\n",
    "\n",
    "# E04: Remove F.one_hot by directly indexing into rows of W\n",
    "class TrigramModelDirect(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=10):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # Embedding tables for each position in the context\n",
    "        self.embeddings0 = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embeddings1 = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Linear layer to produce logits for next character prediction\n",
    "        self.linear = nn.Linear(embedding_dim * 2, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, 2) where each entry is a character index\n",
    "        # Get embeddings for each position in the context\n",
    "        emb0 = self.embeddings0(x[:, 0])  # (batch_size, embedding_dim)\n",
    "        emb1 = self.embeddings1(x[:, 1])  # (batch_size, embedding_dim)\n",
    "        \n",
    "        # Concatenate the embeddings\n",
    "        emb = torch.cat([emb0, emb1], dim=1)  # (batch_size, embedding_dim * 2)\n",
    "        \n",
    "        # Forward through the linear layer to get logits\n",
    "        logits = self.linear(emb)  # (batch_size, vocab_size)\n",
    "        return logits\n",
    "\n",
    "# E05: Using F.cross_entropy\n",
    "def train_model(model, X, Y, lr=0.1, epochs=100):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        logits = model(X)\n",
    "        # Calculate loss using cross_entropy\n",
    "        loss = F.cross_entropy(logits, Y)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate(model, X, Y):\n",
    "    with torch.no_grad():\n",
    "        logits = model(X)\n",
    "        loss = F.cross_entropy(logits, Y)\n",
    "    return loss.item()\n",
    "\n",
    "# Train and evaluate trigram neural model\n",
    "print(\"\\nNeural Network-based Trigram Model:\")\n",
    "print(\"--------------------------------\")\n",
    "trigram_nn = TrigramModelDirect(vocab_size)\n",
    "train_losses = train_model(trigram_nn, X_train, Y_train)\n",
    "\n",
    "# Evaluate on train, dev, and test sets\n",
    "train_loss = evaluate(trigram_nn, X_train, Y_train)\n",
    "dev_loss = evaluate(trigram_nn, X_dev, Y_dev)\n",
    "test_loss = evaluate(trigram_nn, X_test, Y_test)\n",
    "\n",
    "print(f\"Final Train Loss: {train_loss:.4f}\")\n",
    "print(f\"Dev Loss: {dev_loss:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Now let's also train a bigram neural network model for comparison\n",
    "X_train_bigram, Y_train_bigram = build_dataset(train_words, context_size=1)\n",
    "X_dev_bigram, Y_dev_bigram = build_dataset(dev_words, context_size=1)\n",
    "X_test_bigram, Y_test_bigram = build_dataset(test_words, context_size=1)\n",
    "\n",
    "class BigramModelDirect(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=10):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, 1)\n",
    "        emb = self.embedding(x[:, 0])  # (batch_size, embedding_dim)\n",
    "        logits = self.linear(emb)  # (batch_size, vocab_size)\n",
    "        return logits\n",
    "\n",
    "print(\"\\nNeural Network-based Bigram Model:\")\n",
    "print(\"--------------------------------\")\n",
    "bigram_nn = BigramModelDirect(vocab_size)\n",
    "train_losses_bigram = train_model(bigram_nn, X_train_bigram, Y_train_bigram)\n",
    "\n",
    "# Evaluate bigram model\n",
    "bigram_train_loss = evaluate(bigram_nn, X_train_bigram, Y_train_bigram)\n",
    "bigram_dev_loss = evaluate(bigram_nn, X_dev_bigram, Y_dev_bigram)\n",
    "bigram_test_loss = evaluate(bigram_nn, X_test_bigram, Y_test_bigram)\n",
    "\n",
    "print(f\"Bigram Final Train Loss: {bigram_train_loss:.4f}\")\n",
    "print(f\"Bigram Dev Loss: {bigram_dev_loss:.4f}\")\n",
    "print(f\"Bigram Test Loss: {bigram_test_loss:.4f}\")\n",
    "\n",
    "# Compare bigram and trigram neural models\n",
    "print(\"\\nComparison of Neural Network Bigram vs Trigram models:\")\n",
    "print(\"---------------------------------------------------\")\n",
    "print(f\"Bigram Train Loss: {bigram_train_loss:.4f}, Trigram Train Loss: {train_loss:.4f}\")\n",
    "print(f\"Bigram Dev Loss: {bigram_dev_loss:.4f}, Trigram Dev Loss: {dev_loss:.4f}\")\n",
    "print(f\"Bigram Test Loss: {bigram_test_loss:.4f}, Trigram Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# E06: Generate names using our trigram model\n",
    "# -------------------------\n",
    "\n",
    "def generate_name(model, max_length=20):\n",
    "    # Start with two '.' tokens\n",
    "    context = torch.tensor([[0, 0]])  # Shape: (1, 2)\n",
    "    name = ''\n",
    "    \n",
    "    while True:\n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(context)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "        # Sample from the distribution\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        # If we sample the end token, we're done\n",
    "        if ix == 0:\n",
    "            break\n",
    "            \n",
    "        # Add the character to our name\n",
    "        name += itos[ix]\n",
    "        \n",
    "        # Update context for next prediction\n",
    "        context = context[:, 1:]  # Remove the first character\n",
    "        context = torch.cat([context, torch.tensor([[ix]])], dim=1)  # Add the new character\n",
    "        \n",
    "        # Safety check for very long names\n",
    "        if len(name) >= max_length:\n",
    "            break\n",
    "            \n",
    "    return name\n",
    "\n",
    "# Generate 10 names with our trigram model\n",
    "print(\"\\nGenerating names with our trigram model:\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(10):\n",
    "    print(generate_name(trigram_nn))\n",
    "\n",
    "# Let's also try sampling with different \"temperatures\"\n",
    "def generate_name_with_temperature(model, temperature=1.0, max_length=20):\n",
    "    context = torch.tensor([[0, 0]])\n",
    "    name = ''\n",
    "    \n",
    "    while True:\n",
    "        with torch.no_grad():\n",
    "            logits = model(context)\n",
    "            # Apply temperature scaling to logits\n",
    "            logits_temp = logits / temperature\n",
    "            probs = F.softmax(logits_temp, dim=1)\n",
    "            \n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        if ix == 0:\n",
    "            break\n",
    "            \n",
    "        name += itos[ix]\n",
    "        context = context[:, 1:]\n",
    "        context = torch.cat([context, torch.tensor([[ix]])], dim=1)\n",
    "        \n",
    "        if len(name) >= max_length:\n",
    "            break\n",
    "            \n",
    "    return name\n",
    "\n",
    "print(\"\\nGenerating names with different temperatures:\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"Low temperature (more conservative):\")\n",
    "for i in range(5):\n",
    "    print(generate_name_with_temperature(trigram_nn, temperature=0.5))\n",
    "\n",
    "print(\"\\nHigh temperature (more creative):\")\n",
    "for i in range(5):\n",
    "    print(generate_name_with_temperature(trigram_nn, temperature=2.0))\n",
    "\n",
    "# Summary of what we've learned and conclusions\n",
    "print(\"\\nSummary and Conclusions:\")\n",
    "print(\"----------------------\")\n",
    "print(\"1. Trigram models generally perform better than bigram models because they capture more context.\")\n",
    "print(\"2. Proper smoothing is crucial for generalization to unseen data.\")\n",
    "print(\"3. Neural network-based models can learn more complex patterns than simple counting-based models.\")\n",
    "print(\"4. Using embeddings and direct indexing is more efficient than one-hot encoding.\")\n",
    "print(\"5. F.cross_entropy is a more numerically stable and efficient way to compute loss.\")\n",
    "print(\"6. Temperature in sampling allows control over the creativity/randomness of generated names.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
